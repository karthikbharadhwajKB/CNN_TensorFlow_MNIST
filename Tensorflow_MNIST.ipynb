{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_MNIST",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysCPEd1EcVED",
        "colab_type": "text"
      },
      "source": [
        "### Importing Relevant Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDgU_2YRXupq",
        "colab_type": "code",
        "outputId": "9e11638e-c16b-46cc-f997-3521d22a2511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist  import input_data\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmU552iVchVk",
        "colab_type": "code",
        "outputId": "f3f0815c-fd0b-4584-fce5-3998dc90eed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        }
      },
      "source": [
        "mnist = input_data.read_data_sets('MNIST_data/',one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-d08c12c637be>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzUwHH8kcTN9",
        "colab_type": "text"
      },
      "source": [
        "## Outline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_KFdRa6en-o",
        "colab_type": "code",
        "outputId": "e94eb50d-7596-494f-8fcc-10008c45561e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layers = 50\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "#placeholders\n",
        "inputs = tf.placeholder(dtype=tf.float32,shape=(None,input_size))\n",
        "targets = tf.placeholder(dtype=tf.float32,shape=(None,output_size))\n",
        "\n",
        "#weights and bias of 1\n",
        "weights_1 = tf.get_variable(name='Weights_1',shape=(input_size,hidden_layers))\n",
        "bias_1 = tf.get_variable(name='Bias_1',shape=(hidden_layers))\n",
        "\n",
        "#neurons_of_hiddenlayer_1\n",
        "output_1 = tf.nn.relu(tf.matmul(inputs,weights_1) + bias_1)\n",
        "\n",
        "#weights and bias of 2\n",
        "weights_2 = tf.get_variable(name='Weights_2',shape=(hidden_layers,hidden_layers))\n",
        "bias_2 = tf.get_variable(name='Bias_2',shape=(hidden_layers))\n",
        "\n",
        "#neurons_of_hiddenlayer_2\n",
        "outputs_2 = tf.nn.relu(tf.matmul(output_1,weights_2) + bias_2)\n",
        "\n",
        "#weights and bias of 3\n",
        "weights_3 = tf.get_variable(name='Weights_3',shape=(hidden_layers,output_size))\n",
        "bias_3 = tf.get_variable(name='bias_3',shape=(output_size))\n",
        "\n",
        "#final_out = tf.nn.softmax(tf.matmul(outputs_2,weights_3) + bias_3)\n",
        "\n",
        "output = tf.matmul(outputs_2,weights_3) + bias_3\n",
        " \n",
        "\n",
        "loss = tf.nn.softmax_cross_entropy_with_logits(logits=output,labels=targets)\n",
        "\n",
        "mean_loss = tf.reduce_mean(loss)\n",
        "\n",
        "optimize = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)\n",
        "\n",
        "#Accuracy\n",
        "out_eq_targets = tf.equal(tf.arg_max(output,1), tf.arg_max(targets,1))\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(out_eq_targets,tf.float32))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-6372fafb5dc8>:34: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOosCOBQgDEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "\n",
        "intializer = tf.global_variables_initializer()\n",
        "\n",
        "sess.run(intializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjR7yp2cP3pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#batches \n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "batch_no = mnist.train._num_examples // batch_size\n",
        "\n",
        "#Early stopping \n",
        "\n",
        "max_epochs = 15 \n",
        "\n",
        "pre_validation_loss = 9999999"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eELyv2vYTQmJ",
        "colab_type": "text"
      },
      "source": [
        "###Learning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAVS97k-TBvO",
        "colab_type": "code",
        "outputId": "135c8fb5-bc49-475c-f79c-9bdc4a9a5636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        }
      },
      "source": [
        "for epoch_counter in range(max_epochs):\n",
        "  curr_epoch_loss = 0 \n",
        "  for batch_counter in range(batch_no):\n",
        "    input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
        "    _,batch_loss = sess.run([optimize,mean_loss],feed_dict={inputs:input_batch , targets:target_batch})\n",
        "    curr_epoch_loss += batch_loss\n",
        "\n",
        "  curr_epoch_loss /= batch_no\n",
        "  #validation loss \n",
        "  input_batch, target_batch = mnist.validation.next_batch(mnist.validation.num_examples)\n",
        "  validation_loss, validation_accuracy = sess.run([mean_loss,accuracy],feed_dict={inputs:input_batch,targets:target_batch})\n",
        "\n",
        "  print(f'Epoch : {epoch_counter}  \\n Training loss : {curr_epoch_loss}  \\n Validation_loss : {validation_loss} \\n Validation_accuracy : {validation_accuracy}')\n",
        "\n",
        "\n",
        "  if validation_loss > pre_validation_loss:\n",
        "    break \n",
        "\n",
        "  pre_validation_loss = validation_loss \n",
        "\n",
        "print('End of Training')    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0  \n",
            " Training loss : 0.4367204254188321  \n",
            " Validation_loss : 0.20413509011268616 \n",
            " Validation_accuracy : 0.9440000057220459\n",
            "Epoch : 1  \n",
            " Training loss : 0.1889810934798284  \n",
            " Validation_loss : 0.15024878084659576 \n",
            " Validation_accuracy : 0.9592000246047974\n",
            "Epoch : 2  \n",
            " Training loss : 0.14087497570297935  \n",
            " Validation_loss : 0.1270369440317154 \n",
            " Validation_accuracy : 0.9678000211715698\n",
            "Epoch : 3  \n",
            " Training loss : 0.11099609683013775  \n",
            " Validation_loss : 0.12372969090938568 \n",
            " Validation_accuracy : 0.9638000130653381\n",
            "Epoch : 4  \n",
            " Training loss : 0.09533234857361425  \n",
            " Validation_loss : 0.1008647084236145 \n",
            " Validation_accuracy : 0.9685999751091003\n",
            "Epoch : 5  \n",
            " Training loss : 0.08043618150732734  \n",
            " Validation_loss : 0.09293773025274277 \n",
            " Validation_accuracy : 0.9733999967575073\n",
            "Epoch : 6  \n",
            " Training loss : 0.07034297129393301  \n",
            " Validation_loss : 0.09068388491868973 \n",
            " Validation_accuracy : 0.9729999899864197\n",
            "Epoch : 7  \n",
            " Training loss : 0.06095725401135331  \n",
            " Validation_loss : 0.09057121723890305 \n",
            " Validation_accuracy : 0.974399983882904\n",
            "Epoch : 8  \n",
            " Training loss : 0.055211094028147105  \n",
            " Validation_loss : 0.0931013897061348 \n",
            " Validation_accuracy : 0.9714000225067139\n",
            "End of Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y507A2uEcpPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}